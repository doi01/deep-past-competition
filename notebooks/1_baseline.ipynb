{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff63471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\deep-past-competition\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749906a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Akkadian transliteration contains a lot of noise and many unknown words, so\n",
    "    # ByT5, which processes text at the character (byte) level rather than the word level, is the strongest choice.\n",
    "    MODEL_NAME = \"google/byt5-small\" \n",
    "    \n",
    "    # ByT5 tends to produce longer token sequences, but 512 tokens is enough at the sentence level.\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    BATCH_SIZE = 8       # Adjust depending on GPU memory (on a P100 you can usually go with 8–16).\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 2e-4\n",
    "    OUTPUT_DIR = \"./model/byt5-akkadian-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b43dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the seed (for reproducibility).\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf5315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = os.path.abspath(\"../data/raw\")\n",
    "train_df = pd.read_csv(f\"{INPUT_DIR}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{INPUT_DIR}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03b2b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Data: 1561 docs\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Train Data: {len(train_df)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6e5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sentence_aligner(df):\n",
    "    aligned_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        src = str(row['transliteration'])\n",
    "        tgt = str(row['translation'])\n",
    "        \n",
    "        # Split the English text by sentence-ending punctuation.\n",
    "        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n",
    "        \n",
    "        # Assume the Akkadian text is often separated by newlines and split accordingly.\n",
    "        src_lines = [s.strip() for s in src.split('\\n') if s.strip()]\n",
    "        \n",
    "        # If the counts match, trust it as 1-to-1 pairs and use the split version.\n",
    "        if len(tgt_sents) > 1 and len(tgt_sents) == len(src_lines):\n",
    "            for s, t in zip(src_lines, tgt_sents):\n",
    "                if len(s) > 3 and len(t) > 3: # Remove junk/noisy data.\n",
    "                    aligned_data.append({'transliteration': s, 'translation': t})\n",
    "        else:\n",
    "            # If splitting fails (counts don't match), keep the original document pair as-is (safe fallback).\n",
    "            aligned_data.append({'transliteration': src, 'translation': tgt})\n",
    "            \n",
    "    return pd.DataFrame(aligned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Train Data: 1561 sentences (Alignment applied)\n"
     ]
    }
   ],
   "source": [
    "# Run data augmentation.\n",
    "train_expanded = simple_sentence_aligner(train_df)\n",
    "print(f\"Expanded Train Data: {len(train_expanded)} sentences (Alignment applied)\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format & split into Train/Val.\n",
    "dataset = Dataset.from_pandas(train_expanded)\n",
    "# Create a validation set with test_size=0.1.\n",
    "split_datasets = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "# After splitting, the keys are 'train' and 'test' (we'll use 'test' as validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6f0f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Map: 100%|██████████| 1404/1404 [00:28<00:00, 49.26 examples/s]\n",
      "Map: 100%|██████████| 157/157 [00:03<00:00, 49.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. Tokenization & preprocessing\n",
    "# ==========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "\n",
    "# Fix the corresponding section in dpc-starter-train.\n",
    "PREFIX = \"translate Akkadian to English: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + str(ex) for ex in examples[\"transliteration\"]]\n",
    "    targets = [str(ex) for ex in examples[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=Config.MAX_LENGTH, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=Config.MAX_LENGTH, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = split_datasets[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_val = split_datasets[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45337f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Model training (fine-tuning)\n",
    "# ==========================================\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(Config.MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Metric (chrF++ is part of the competition metric and measures character-level precision/overlap).\n",
    "metric = evaluate.load(\"chrf\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple): preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Ignore -100 in the labels.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"chrf\": result[\"score\"]}\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=Config.OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=Config.LEARNING_RATE,\n",
    "    \n",
    "    # === Key fixes ===\n",
    "    fp16=False,                     # ★Set to False to prevent a NaN error (required).\n",
    "    per_device_train_batch_size=4,  # ★fp32 uses more memory, so reduce the batch size (8 -> 4).\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,  # ★To compensate, accumulate gradients to keep the effective batch size at 8.\n",
    "    # ======================\n",
    "    \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=Config.EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=10,               # Inspect logs in more detail.\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting Training (FP32 mode)...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(Config.OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(Config.OUTPUT_DIR)\n",
    "print(f\"Model saved to {Config.OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
